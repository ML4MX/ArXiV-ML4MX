# DataSets


# Vision

* [MNIST](http://yann.lecun.com/exdb/mnist/)  
  * 
* [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html)
  * 
* [The Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/)
  * 
* [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist/blob/master/README.md)
  * 
  * 
* [Open Images dataset](https://github.com/openimages/dataset)
  * 
  * 
  * 
* [Food-101](https://www.vision.ee.ethz.ch/datasets_extra/food-101/)
  * 
  * 
* [Columbia University Image Library (COIL-20)](http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php)
  * 
  * 

## Video

* [YouTube-8M](https://research.google.com/youtube8m/)
  * 
* [YouTube-BoundingBoxes](https://research.google.com/youtube-bb/)
  * 
* [Moments in Time Dataset](http://moments.csail.mit.edu/)
  * 
  * 
* [Kinetics](https://deepmind.com/research/open-source/open-source-datasets/kinetics/)
  * 
* [Atomic Visual Actions (AVA)](https://research.google.com/ava/)
  * 
  * [AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions](https://arxiv.org/abs/1705.08421)
* [20BN-JESTER/ 20BN-SOMETHING-SOMETHING](https://www.twentybn.com/datasets)
  * 
* [TRECVID datasets](http://www-nlpir.nist.gov/projects/trecvid/past.data.table.html)
  * 
  * 
* [UCF101](http://crcv.ucf.edu/data/UCF101.php)
  * 
  * 
* [Playing for Benchmarks](http://playing-for-benchmarks.org/)
  * 
  * 


## Scene

* [SceneNet RGB-D](https://robotvault.bitbucket.io/scenenet-rgbd.html)
  * 
  * 
* [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/)
  * 
  * [Semantic Understanding of Scenes through the ADE20K Dataset](https://github.com/arXivTimes/arXivTimes/issues/291)
* [Places365](http://places2.csail.mit.edu/download.html)
  * 
  * 
  * 
* [KITTI](http://www.cvlibs.net/datasets/kitti/)
  * 
  * 
* [Mapillary Vistas Dataset](https://www.mapillary.com/dataset/vistas)
  * 
* [Describable Textures Dataset (DTD)](https://www.robots.ox.ac.uk/~vgg/data/dtd/)
  *
  * 
* [SUN database](http://groups.csail.mit.edu/vision/SUN/)
  * 
  * 
  * 
* [Team MC^2 : ARC RGB-D Dataset 2017](http://mprg.jp/research/arc_dataset_2017_j)
  * 
  * 
  * 
  * 
  * 
* [Matterport3D: Learning from RGB-D Data in Indoor Environments](https://niessner.github.io/Matterport/)
  * 
  * 
* [3D_Street_View](https://github.com/amir32002/3D_Street_View)
  * 
  * 
* [The German Traffic Sign Detection Benchmark](http://benchmark.ini.rub.de/?section=gtsdb&subsection=dataset)
  * 
  * 
  * 

## 3D

* [ScanNet](http://www.scan-net.org/)
  * 
* [ShapeNet](http://shapenet.cs.stanford.edu/)
  * 
  * 
* [ModelNet](http://modelnet.cs.princeton.edu/)
  * [SUN database](http://sun.cs.princeton.edu/)
  * 
* [SHREC 2014](http://www.itl.nist.gov/iad/vug/sharp/contest/2014/Generic3D/)
  * 
  * 
  * 
* [Yobi3D](https://www.yobi3d.com/)
  * 

## Satellite

* [SpaceNet](https://github.com/SpaceNetChallenge/utilities/tree/master/content/download_instructions)
  * 
  * 
* [ABCD (AIST Building Change Detection) dataset](https://github.com/faiton713/ABCDdataset)
  * 
  * 
* [Dublin LiDAR dataset](https://geo.nyu.edu/catalog?f%5Bdct_isPartOf_sm%5D%5B%5D=2015+Dublin+LiDAR)
  * 
  * 

## BodyParts

* [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)
  * 
* [MegaFace and MF2: Million-Scale Face Recognition](http://megaface.cs.washington.edu/)
  * 
  * 
* [11k Hands](https://sites.google.com/view/11khands)
  * 
  * 
  * 
* [AISL HDIBPL (Human Depth Images with Body Part Labels) Database](http://www.aisl.cs.tut.ac.jp/database_HDIBPL.html)
  * 
  * 
* [The Event-Camera Dataset and Simulator](http://rpg.ifi.uzh.ch/davis_data.html)
  * 
  * 
  * [slides](http://www.rit.edu/kgcoe/iros15workshop/papers/IROS2015-WASRoP-Invited-04-slides.pdf),
* [MPI Dynamic FAUST(D-FAUST)](http://dfaust.is.tue.mpg.de/)
  * 

## Medical

* [Annotated lymph node CT data](https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+Nodes)
  *
* [Annotated pancreas CT data](https://wiki.cancerimagingarchive.net/display/Public/Pancreas-CT)
  * 
* [Chest radiograph dataset](https://nihcc.app.box.com/v/ChestXray-NIHCC)
  * 
  * [ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases](http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf)
  * 

## Art

* [Painter by Numbers(PBN)](https://www.kaggle.com/c/painter-by-numbers/data)
  * 
  * 
* [quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)
  * 
  * [sketch-rnn-datasets](https://github.com/hardmaru/sketch-rnn-datasets)
  * [model](https://github.com/tensorflow/magenta/tree/master/magenta/models/sketch_rnn)
  * [A Neural Representation of Sketch Drawings](https://arxiv.org/abs/1704.03477)
* [Manga109](http://www.manga109.org/ja/)
  * 
  * 
* [eBDtheque](http://ebdtheque.univ-lr.fr/)
  * 
  * 
  * 
* [AnimeFace Character Dataset](http://www.nurs.or.jp/~nagadomi/animeface-character-dataset/README.html)
  * 
* [GDI (Graphic Design Importance) Dataset](http://www.dgp.toronto.edu/~donovan/layout/index.html)
  * 
  * [BubbleView](https://namwkim.github.io/bubbleview/)
  * [model](https://github.com/cvzoya/visimportance/tree/master/data)
* [LLD - Large Logo Dataset](https://data.vision.ee.ethz.ch/cvl/lld/)
  * 
  * 
  * [Favicons](https://www.kaggle.com/colinmorris/favicons)
* [MASSVIS DATASET](http://massvis.mit.edu/)
  * 
  * 
  * 
* [AADB dataset](https://github.com/aimerykong/deepImageAestheticsAnalysis)
  * 
  * 
  * [Photo Aesthetics Ranking Network with Attributes and Content Adaptation](https://arxiv.org/abs/1606.01621)

## Captioning

* [VQA](http://www.visualqa.org/index.html)
  * (Zero-Shot)
  * [Zero-Shot Visual Question Answering](https://arxiv.org/abs/1611.05546)
* [CLEVR](http://cs.stanford.edu/people/jcjohns/clevr/)
  * 
  * 
* [MS COCO](http://mscoco.org/home/)
  * 
  * 
* [COCO-Stuff 10K](https://github.com/nightrome/cocostuff)
  * COCO
* [VisDial Dataset](https://visualdialog.org/data)
  * MS COCO Training 8, Validation 4
  * [site](https://github.com/batra-mlp-lab/visdial-amt-chat)
* [STAIR Captions](https://stair-lab-cit.github.io/STAIR-captions-web/)
  * MS COCO
  * [STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset](https://arxiv.org/abs/1705.00823)
* [Cornell NLVR](http://lic.nlp.cornell.edu/nlvr/)
  * 
  * [A Corpus of Natural Language for Visual Reasoning](http://alanesuhr.com/suhr2017.pdf)
* [Recipe1M dataset](http://im2recipe.csail.mit.edu/)
  * 
  * 


# NLP

* [site](http://nlp.ist.i.kyoto-u.ac.jp/index.php?NLP%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9#g63a7f30)
  * [site](http://nlp.ist.i.kyoto-u.ac.jp/index.php?%E4%BA%AC%E9%83%BD%E5%A4%A7%E5%AD%A6%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9) [site](http://nlp.ist.i.kyoto-u.ac.jp/index.php?KWDLC)
* [site](http://nlp.ist.i.kyoto-u.ac.jp/kuntt/)
  * 
* [Stanford Rare Word (RW) Similarity Dataset](https://nlp.stanford.edu/~lmthang/morphoNLM/)
  * 
  * 
  * [site](https://medium.com/@taher.pilevar/is-the-stanford-rare-word-similarity-dataset-a-reliable-evaluation-benchmark-3fe409053011)
* [(JapaneseWordSimilarityDataset)](https://github.com/tmu-nlp/JapaneseWordSimilarityDataset)
  * Stanford Rare Word Similarity Dataset
  * 
* [WikiText](https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/)
  * WikiText-2/WikiText-103 Penn Treebank 2&110
* [WikiSQL](https://github.com/salesforce/WikiSQL)
  * SQL (SELECT)
  * SQL
* [青空文庫](http://www.aozora.gr.jp/)
  * XHTML(一HTML
* [青空文庫形態素解析データ集](http://aozora-word.hahasoha.net/index.html)
  * 
  * 
* [BookCorpus](http://yknzhu.wixsite.com/mbweb)
  * 10,000
  * 
* [csi-corpus](https://github.com/EdinburghNLP/csi-corpus)
  * 
  * 
  * [Whodunnit? Crime Drama as a Case for Natural Language Understanding](https://arxiv.org/pdf/1710.11601.pdf)

## Parallel Corpus

* [日本語対訳データ](http://phontron.com/japanese-translation-data.php?lang=ja)
* [Tanaka Corpus](http://www.edrdg.org/wiki/index.php/Tanaka_Corpus)
  * 
  * ([small_parallel_enja](https://github.com/odashi/small_parallel_enja))。
* [JESC: Japanese-English Subtitle Corpus](http://cs.stanford.edu/~rpryzant/jesc/)
  * 
  * 320
  * [JESC: Japanese-English Subtitle Corpus](https://arxiv.org/abs/1710.10639)

## Classification

* [livedoor](https://www.rondhuit.com/download.html)
  * Sports
* [SemEval-2017 Task 8 RumourEval](http://alt.qcri.org/semeval2017/task8/)
  * 
  * (Support)・(Deny)・(Query)・(Comment)
* [PHEME rumour dataset: support, certainty and evidentiality](https://www.pheme.eu/2016/06/13/pheme-rumour-dataset-support-certainty-and-evidentiality/)
  * 
  * (support)・(certainty)・(evidentiality)
* [Noun Compositionality Judgements](https://www.kaggle.com/rtatman/noun-compositionality-judgements)
  * (literal or not literal)
  * red apple, search engine
* [Enron Email Dataset](https://www.cs.cmu.edu/~enron/)
  * 
  * 
  * ([EmailIntentDataSet](https://github.com/ParakweetLabs/EmailIntentDataSet))
* [PubMed 200k RCT dataset](https://github.com/Franck-Dernoncourt/pubmed-rct)
  * 
  * 
  * [PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts](https://arxiv.org/abs/1710.06071)

## Sentiment

* [Sentiment Treebank](https://nlp.stanford.edu/sentiment/code.html)
  * Stanford
* [Crowdflower](https://www.kaggle.com/crowdflower/datasets)
  * Crowdflower, Twitter 
* [PersonaBank](https://nlds.soe.ucsc.edu/personabank)
  * 
  * 
* [MovieLens 1M Dataset](https://grouplens.org/datasets/movielens/1m/)
  * MovieLens
  * 
* [MovieTweetings](https://github.com/sidooms/MovieTweetings)
  * IMDB ("I rated The Matrix 9/10 http://www.imdb.com/title/tt0133093/ #IMDb")
  * Twitter
* [A Large Self-Annotated Corpus for Sarcasm](https://arxiv.org/pdf/1704.05579.pdf)
  * Reddit 
  * [site](http://nlp.cs.princeton.edu/SARC/)
* [SemEval-2016 Task 5: Aspect-Based Sentiment Analysis](http://alt.qcri.org/semeval2016/task5/)		
  * 
  * 
  * 
* [Amazon product data](http://jmcauley.ucsd.edu/data/amazon/)
  * 
  * 

## Entity Recognition

* [WNUT17 Emerging and Rare entity recognition](https://noisy-text.github.io/2017/emerging-rare-entities.html)
  * SNS
  * 
* [WNUT Named Entity Recognition](https://github.com/aritter/twitter_nlp/tree/master/data/annotated/wnut16)
  * Twitter SNS
* [W-NUT Geolocation Prediction in Twitter](https://noisy-text.github.io/2016/geo-shared-task.html)
  * Twitter
  * User-level Message-level
* [Automated Analysis of Cybercriminal Markets](https://evidencebasedsecurity.org/forums/)
  *
  * (Hack Forums), (Blackhat)

## Knowledge Base

* [Visual Genome](http://visualgenome.org/)
  * 
* [Microsoft Concept Graph](https://concept.research.microsoft.com/Home/Introduction)
  * Microsoft

## Q&A

* [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)
  * Stanford
* [Maluuba News QA](http://datasets.maluuba.com/NewsQA)
  * CNN
* [MS MARCO](http://www.msmarco.org/)
  * Microsoft
  * [MS MARCO: A Human Generated MAchine Reading COmprehension Dataset](https://arxiv.org/pdf/1611.09268v1.pdf)
* [TriviaQA: A Large Scale Dataset for Reading Comprehension and Question Answering](http://nlp.cs.washington.edu/triviaqa/)
  * Evidence(Answer Web Wikipedia)
  * SQuAD
  * [TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension](http://nlp.cs.washington.edu/triviaqa/docs/triviaQA.pdf)
* [WebQuestions/Free917](https://nlp.stanford.edu/software/sempre/)
  * 5W(When/Where/Who/What/Why)
  * WebQuestions Free
* [TREC QA](http://trec.nist.gov/data/qa.html)
  * 1999
* [DeepMind Q&A Dataset](http://cs.nyu.edu/~kcho/DMQA/)
  * CNN/Daily Mail
  * Stanford [Machine Comprehension](https://www.slideshare.net/takahirokubo7792/machine-comprehension)
* [QAngaroo](http://qangaroo.cs.ucl.ac.uk/)
  * 
  * [Constructing Datasets for Multi-hop Reading Comprehension Across Documents](https://arxiv.org/abs/1710.06481)
* [FigureQA](https://datasets.maluuba.com/FigureQA)
  * 
* [The NarrativeQA Reading Comprehension Challenge Dataset](https://github.com/deepmind/narrativeqa)
  * Machine Reading
  * Q&A
  * 


## Dialog

* [The Ubuntu Dialogue Corpus](https://github.com/rkadlec/ubuntu-ranking-dataset-creator)
  * Ubuntu
* [E2E NLG](http://www.macs.hw.ac.uk/InteractionLab/E2E/)
  * End-to-End
  * [The E2E Dataset: New Challenges For End-to-End Generation](https://arxiv.org/abs/1706.09254)
* [A Multi-Turn, Multi-Domain, Task-Oriented Dialogue Dataset](https://nlp.stanford.edu/blog/a-new-multi-turn-multi-domain-task-oriented-dialogue-dataset/)
  * 
  * 
  * 
* [bAbI](https://research.fb.com/downloads/babi/)
  * Facebook AI Research
  * 
* [self_dialogue_corpus](https://github.com/jfainberg/self_dialogue_corpus)
  * Amazon Alexa Prize
  * 
  * 
  * [Edina: Building an Open Domain Socialbot with Self-dialogues](https://arxiv.org/abs/1709.09816)

## Reasoning

* [HolStep](http://cl-informatik.uibk.ac.at/cek/holstep/)
  * Google
  * [HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving](https://arxiv.org/abs/1703.00426)
* [SCONE: Sequential CONtext-dependent Execution dataset](https://nlp.stanford.edu/projects/scone/)
  * Stanford
  * 
  * [Simpler Context-Dependent Logical Forms via Model Projections](https://arxiv.org/abs/1606.05378)

## Summarization/Correction

* [DUC 2004](http://www.cis.upenn.edu/~nlp/corpora/sumrepo.html)
  * 
* [boxscore-data](http://lstm.seas.harvard.edu/docgen/)
  * 
  * Rotowire/SBNation
* [AESW](http://textmining.lt/aesw/index.html)
  * 
  * 
* [Lang-8 dataset](http://cl.naist.jp/nldata/lang-8/)
  * 
  * 
  * [site](https://www.ninjal.ac.jp/event/specialists/project-meeting/files/JCLWorkshop_no6_papers/JCLWorkshop_No6_27.pdf)


# Audio

* [DCASE](http://www.cs.tut.fi/sgn/arg/dcase2016/task-acoustic-scene-classification)
  * 
* [Freesound 4 seconds](https://archive.org/details/freesound4s)
  * FreeSound
* [AudioSet](https://research.google.com/audioset/)
  * YouTube
* [NSynth Dataset](https://magenta.tensorflow.org/nsynth)
  * 
* [Yamaha e-Piano Competition dataset](http://www.piano-e-competition.com/midi_2004.asp)
  * [Performance RNN](https://magenta.tensorflow.org/performance-rnn)
* [The Largest MIDI Collection on the Internet](https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_internet/)
  * 
  * 
  * Tronto [Song From PI](http://www.cs.toronto.edu/songfrompi/)
* [The MagnaTagATune Dataset](http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset)
  * [TagATune](http://www.cs.cmu.edu/~elaw/papers/tagatune.pdf), [Magnatune](http://magnatune.com/)
* [site](http://voice-statistics.github.io/)
  * 
  * 48kHz/16bit WAV 720MB。
* [JSUT(Japanese speech corpus of Saruwatari Lab, University of Tokyo)](https://sites.google.com/site/shinnosuketakamichi/publication/jsut)
  * 48kH。
  *
* [Speech Commands Dataset](https://www.tensorflow.org/versions/master/tutorials/audio_recognition)
  * TensorFlow
  * 
* [The Spoken Wikipedia Corpora](http://nats.gitlab.io/swc/)
  * Wikipedia
  * 
* [Common Voice](https://voice.mozilla.org/data)
  * Mozilla
  * [DeepSpeech](https://github.com/mozilla/DeepSpeech)

# Other

* [grocery-shopping-2017](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)
  * Instacart
* [site](http://www.data.jma.go.jp/gmd/risk/obsdl/index.php)
  * 
* [Global Terrorism Database](https://www.kaggle.com/START-UMD/gtd)
  * 1970~2016
  * START
* [THE STANFORD OPEN POLICING PROJECT](https://openpolicing.stanford.edu/)
  * 
  * 

## Chemical/Medical

* [MoleculeNet](https://arxiv.org/abs/1703.00564)
  * MoleculeNet
  * [DeepChem](https://github.com/deepchem/deepchem)
* [Tox21](https://tripod.nih.gov/tox21/challenge/data.jsp)
  * (toxic effects)
* [dSPP: Database of structural propensities of proteins](https://peptone.io/dspp)
  * (structural propensity score)
  * Keras ([dspp-keras](https://github.com/PeptoneInc/dspp-keras))
* [MIMIC](https://mimic.physionet.org/)
  * 40,000
  * CITI "Data or Specimens Only Research ([site](https://mimic.physionet.org/gettingstarted/access/)

## Security

* [SARD Dataset](https://samate.nist.gov/SRD/testsuite.php)
  * SARD(Software Assurance Reference Dataset)
  * 
* [PHP Security vulnerability dataset](https://seam.cs.umd.edu/webvuldata/data.html)
  * PHP (CVE ID)
* [Passwords](https://wiki.skullsecurity.org/Passwords)
  * 

## Game

* [GoGoD](http://senseis.xmp.net/?GoGoDCD)
  * 15USD
* [wangjinzhuo/pgd](https://github.com/wangjinzhuo/pgd)
  *
* [TorchCraft/StarData](https://github.com/TorchCraft/StarData)
  * StarCraft

# Dataset Summary Page

* [kaggle](https://www.kaggle.com/)
  * [Kaggle Datasets](https://www.kaggle.com/datasets) [site](http://codh.rois.ac.jp/)
  * 
  * 
* [site](http://www.nii.ac.jp/dsc/idr/datalist.html)
  * 
* [Harvard Dataverse](https://dataverse.harvard.edu/)
  * 
* [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html)
  * 
* [20 Weird & Wonderful Datasets for Machine Learning](https://medium.com/@olivercameron/20-weird-wonderful-datasets-for-machine-learning-c70fc89b73d5)
  * 
* [site](http://qiita.com/icoxfog417/items/44aeb9486ce1b7130f76)
* [Microsoft Azure Marketplace](https://datamarket.azure.com/browse/data)
  * NFL
* [ikegami-yukino/dataset-list](https://github.com/ikegami-yukino/dataset-list/blob/master/free_corpus.md)
  * 
* [beamandrew/medical-data](https://github.com/beamandrew/medical-data)
  * 
* [Web Technology and Information Systems](https://www.uni-weimar.de/en/media/chairs/computer-science-and-media/webis/corpora/)
  * Web Technology and Information Systems


# To make your own

* [site](https://www.amazon.co.jp/dp/4061529137)
* [Crowdsourcing (for NLP)](http://veredshwartz.blogspot.jp/2016/08/crowdsourcing-for-nlp.html)
  * 
* [Natural Language Annotation for Machine Learning](http://shop.oreilly.com/product/0636920020578.do)
* [site](https://www.amazon.co.jp/dp/4873116406)
* [site](http://garapon.tv/developer/)
  * API
